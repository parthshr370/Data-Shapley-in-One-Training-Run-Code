{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj57KTPHqDvU",
        "outputId": "87883c76-ed72-4023-a855-19a4f2ece72a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sZkwLkvepsaE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import shap\n",
        "from torch.utils.data import DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "yafOL_ecqCwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ae4020-bc0a-40e8-9ce5-def437199ec2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c4e2c7e72f0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device and model parameters\n",
        "device = 'cpu'  # Options: 'cpu', 'cuda' (for GPU)\n",
        "batch_size = 200\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "yl4xLPIJQOks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model, using a simple MLP for MNIST\n",
        "class MLP_MNIST(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_MNIST, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 512)\n",
        "        self.fc2 = torch.nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QrzObFalAfWr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Self\n",
        "\n",
        "# added softmax\n",
        "# converts them into soft probablity curve\n",
        "\n",
        "\n",
        "class MLP_MNIST_Prob(torch.nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(MLP_MNIST_Prob, self).__init__()\n",
        "        self.base_model = base_model  # your original MLP_MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.base_model(x)\n",
        "        probs = F.softmax(logits, dim=1)  # convert to probabilities\n",
        "        return probs"
      ],
      "metadata": {
        "id": "Ux-IBAJ6RcD3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model def and other things def"
      ],
      "metadata": {
        "id": "vn3unuZnPqoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "model = MLP_MNIST().to(device)\n",
        "model_probs = MLP_MNIST_Prob(model).to(device)  # Wrap original trained model"
      ],
      "metadata": {
        "id": "I7qMbyIBZFOE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dont forget the the optims and the loss"
      ],
      "metadata": {
        "id": "JvSxEomvPtF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "jx5qnh4Dc1Uc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the MNIST data"
      ],
      "metadata": {
        "id": "tbguHUhLPwBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "mkkOEG3JdBKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edac2204-f8c2-43b9-fa6f-9707183871f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.46MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.64MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train and test oh shit"
      ],
      "metadata": {
        "id": "bcIyLfHTPynm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "_iVy-TTIdEgw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training loop"
      ],
      "metadata": {
        "id": "-8-xBz42P1ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (simplified training loop with logs for demonstration)\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()  # Summing loss for averaging later\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOUvSoBJd0-n",
        "outputId": "d093cd4a-7574-4e6e-fd5b-866d842bb4fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.3609732197225094\n",
            "Epoch 2, Average Loss: 0.15076458789408206\n",
            "Epoch 3, Average Loss: 0.09985638581216336\n",
            "Epoch 4, Average Loss: 0.07359100867062807\n",
            "Epoch 5, Average Loss: 0.05559881423289577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, SHAP picks or averages across one or more baseline samples (the background data) to estimate how the model behaves when a particular feature is “missing” (or replaced by a baseline/average value) vs. when it is present at its actual value in the data point you want to explain."
      ],
      "metadata": {
        "id": "Ri_Qs1EVieOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The background data usually comes from a subset of the training data (or another representative dataset) to reflect the model’s typical inputs.\n",
        "For images, these samples often represent an average or “typical” distribution of pixel values. For tabular data, these samples represent typical feature values."
      ],
      "metadata": {
        "id": "9TL_1OaYijA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a subset of data for Shapley value computation\n",
        "background_data = Subset(train_dataset, indices=np.random.choice(len(train_dataset), 1000, replace=False))\n",
        "background_loader = DataLoader(background_data, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "3qnTJgsxd8HE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample cuz we cant handle the whole thing"
      ],
      "metadata": {
        "id": "ZiNOORigQvRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a background dataset for Deep SHAP\n",
        "background_samples = torch.cat([data[0].unsqueeze(0) for data, _ in background_loader], 0).to(device)"
      ],
      "metadata": {
        "id": "0K8WmsEffmvR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPVjs4m5SbPk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize SHAP Deep Explainer\n",
        "e = shap.DeepExplainer(model_probs, background_samples)\n",
        "\n",
        "\n",
        "data_point, _ = next(iter(test_loader))  # Get a batch from the test loader\n",
        "data_point = data_point.to(device)       # move data to the appropriate device\n",
        "data_point = data_point[0:1]             # Select the first example from the batch"
      ],
      "metadata": {
        "id": "kmXTA5XqkYSN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option to choose between DeepExplainer and GradientExplainer\n",
        "try:\n",
        "    e = shap.DeepExplainer(model_probs, background_samples)\n",
        "    shap_values = e.shap_values(data_point)\n",
        "except AssertionError as error:\n",
        "    print(\"Switching to GradientExplainer due to error:\", error)\n",
        "    e = shap.GradientExplainer(model, background_samples)\n",
        "    shap_values = e.shap_values(data_point)\n",
        "\n",
        "\n",
        "# Adjust the shape of the data point for visualization\n",
        "data_point = data_point.squeeze()  #"
      ],
      "metadata": {
        "id": "BUQt6xiIkfX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c696c25-8625-449f-b614-f0054f233f2c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switching to GradientExplainer due to error: The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 0.11429014255535921 - Tolerance: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure data_point is reshaped correctly for grayscale visualization\n",
        "if data_point.ndim == 3:\n",
        "    data_point = data_point.permute(1, 2, 0)  # Convert from [C, H, W] to [H, W, C] if needed\n",
        "\n",
        "# Check and adjust dimensions for shap_values if necessary\n",
        "shap_values = [sv.squeeze() for sv in shap_values]  # Remove unnecessary batch dimension"
      ],
      "metadata": {
        "id": "bU0UMKbikh2d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.image_plot(shap_values, -data_point.cpu().numpy())"
      ],
      "metadata": {
        "id": "iO6-pklakkHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "06f6027e-6874-4c83-e0fe-a4c3674e1b31"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap of SHAP values indicating how each pixel influences the model’s prediction.\n",
        "\n",
        "**Positive (Red) Regions:** Pixels that push the model’s output toward the predicted class. Think of them as strong “evidence” in favor of the classification (here, a ‘7’).\n",
        "\n",
        "\n",
        "\n",
        "**Negative (Blue) Regions:** Pixels that push the model’s output away from the predicted class. These areas, if changed, would make the model less likely to predict the digit as a ‘7’."
      ],
      "metadata": {
        "id": "lCL3fOI9dStE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In Run Data Shapley"
      ],
      "metadata": {
        "id": "hNiwbROuUiG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reloading the previous model -"
      ],
      "metadata": {
        "id": "ta7AKbi_p9G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class MLP_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP_MNIST, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "model = MLP_MNIST().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ptVkxlsEp_aP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                           transform=transform, download=True)\n",
        "val_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                         transform=transform, download=True)\n"
      ],
      "metadata": {
        "id": "W5cvk9OvNx1x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# Just pick a small subset of validation data for the demonstration\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "val_data_iter = iter(val_loader)\n",
        "# chosing the validation samples in the dataset\n",
        "val_img, val_label = next(val_data_iter)\n",
        "val_img, val_label = val_img.to(device), val_label.to(device)"
      ],
      "metadata": {
        "id": "rIYhJZW_pqpZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_shapley_scores[i] will hold the in-run shapley score for\n",
        "the i-th sample in the entire train_dataset (index i in train_dataset)"
      ],
      "metadata": {
        "id": "t87CUB4bpxM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_shapley_scores = torch.zeros(len(train_dataset), device=device) # initiate an empty array\n",
        "\n",
        "\n",
        "epochs = 1\n",
        "global_step= 0"
      ],
      "metadata": {
        "id": "yqqZIdTVpszB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dot product is a first-order approximation of how much the loss on the validation sample changes when a training sample is used to update the parameters."
      ],
      "metadata": {
        "id": "x0DL2m5jsADN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do one forward pass on the batch+val, and one backward pass. During this pass, we use hooks to capture:\n",
        "\n",
        "    Each sample’s input xixi​ arriving at the layer (a forward hook).\n",
        "    Each sample’s output gradient δiδi​ leaving the layer (a backward hook)."
      ],
      "metadata": {
        "id": "kOYirmyyqIdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooks in pytorch are basically capturing information from point to point\n",
        "\n",
        "we apply hook after we pass values thorugh logits\n",
        "and we do while backprop"
      ],
      "metadata": {
        "id": "x50u1gb7qd9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# 4. Training Loop with In-Run Data Shapley\n",
        "# ------------------------------------------\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # We create a combined batch:\n",
        "        #   1) the normal training batch\n",
        "        #   2) plus the single validation sample\n",
        "        # to do \"ghost dot-products\" in ONE pass.\n",
        "        # creates a 1D array to combine the images and validation set hence making it ghost dot product\n",
        "\n",
        "        combined_imgs = torch.cat([images, val_img], dim=0)\n",
        "        combined_labels = torch.cat([labels, val_label], dim=0)\n",
        "\n",
        "    # Compute the loss on the combined set\n",
        "        logits = model(combined_imgs) #\n",
        "        train_batch_size = images.size(0)\n",
        "\n",
        "    # The first part of logits is training data, the last part is for val data\n",
        "        loss_train = loss_fn(logits[:train_batch_size], labels)\n",
        "        loss_val   = loss_fn(logits[train_batch_size:], val_label)\n",
        "\n",
        "        # Combined loss (we sum them so that backprop will let us\n",
        "        # access both training-sample gradients and val-sample gradient)\n",
        "        combined_loss = loss_train + loss_val\n",
        "\n",
        "\n",
        "\n",
        "# Ghost dot product for backward pass\n",
        "        optimizer.zero_grad()\n",
        "        combined_loss.backward(retain_graph=True) # retain graph since we need more of the backprop later on\n",
        "\n",
        "        saved_state = {} # save the current sate\n",
        "\n",
        "\n",
        "# We store the current gradients (which are for train+val) in saved_state.\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                # save param.grad clone for later use\n",
        "                saved_state[name] = param.grad.detach().clone()\n",
        "\n",
        "\n",
        "\n",
        "# zero the model gradient\n",
        "# only use val for back/forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits_val_only = model(val_img)\n",
        "        loss_val_only = loss_fn(logits_val_only, val_label)\n",
        "        loss_val_only.backward(create_graph=False)  # single-sample gradient\n",
        "        grad_val = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_val[name] = param.grad.detach().clone()\n",
        "\n",
        "\n",
        "\n",
        "        # restore the combined grad from saved_state\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in saved_state:\n",
        "                param.grad = saved_state[name] # now when it is in param save it to saved_state\n",
        "\n",
        "\n",
        "        # Now let's compute the \"dot product\" for each sample in the train batch:\n",
        "        for i in range(train_batch_size):\n",
        "            # zero grad\n",
        "            param_zero = {}\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad.zero_()\n",
        "\n",
        "            # backward on just the i-th example in the train batch\n",
        "            single_logit = model(images[i].unsqueeze(0))\n",
        "            single_loss = loss_fn(single_logit, labels[i].unsqueeze(0))\n",
        "            single_loss.backward()\n",
        "\n",
        "            # read off dot-product with grad_val\n",
        "            dot_val = 0.0\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None and name in grad_val:\n",
        "                    # flatten both param.grad and grad_val[name]\n",
        "                    dot_val += (param.grad.view(-1) * grad_val[name].view(-1)).sum()\n",
        "\n",
        "            # Add to data_shapley_scores\n",
        "            idx_in_full_dataset = batch_idx*batch_size + i  # approximate global index\n",
        "            data_shapley_scores[idx_in_full_dataset] += -0.01 * dot_val.item() # mul by lr to scale back things\n",
        "\n",
        "\n",
        "            # We used lr=0.01 -> so multiply by -lr. Negative sign because the loss change is -( grad_val · grad_train_i ).\n",
        "\n",
        "# Final logging\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        global_step += 1\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Step {batch_idx}, combined_loss = {combined_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "hPoiGXPiqbep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec64ddc-d75d-43e1-f39f-16097ece0705"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Step 0, combined_loss = 3.2223\n",
            "Epoch 0, Step 100, combined_loss = 5.5803\n",
            "Epoch 0, Step 200, combined_loss = 1.4972\n",
            "Epoch 0, Step 300, combined_loss = 2.8802\n",
            "Epoch 0, Step 400, combined_loss = 0.9619\n",
            "Epoch 0, Step 500, combined_loss = 2.4942\n",
            "Epoch 0, Step 600, combined_loss = 2.7236\n",
            "Epoch 0, Step 700, combined_loss = 5.3820\n",
            "Epoch 0, Step 800, combined_loss = 2.8438\n",
            "Epoch 0, Step 900, combined_loss = 2.2997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Is This “Ghost”?\n",
        "\n",
        "    We never explicitly built a huge per-sample gradient tensor. Instead, we stored only a few intermediate tensors (xi(l)xi(l)​ and δi(l)δi(l)​ for each sample).\n",
        "\n",
        "    this helps us store info on the fly"
      ],
      "metadata": {
        "id": "T7abmfXLq45t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"In-Run Data Shapley (first-order approx) computed for each training sample.\")\n",
        "print(\"data_shapley_scores shape:\", data_shapley_scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiAzc9Nom4hv",
        "outputId": "7e92b5ce-af6d-4603-8316-6edd7f3bf880"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-Run Data Shapley (first-order approx) computed for each training sample.\n",
            "data_shapley_scores shape: torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For each training example, you are estimating a “first-order approximation” of how much that example contributes (positively or negatively) to reducing the validation sample’s loss.\n",
        "\n",
        "- By the end, you’ll have data_shapley_scores[i] telling you whether sample i is beneficial (a positive contribution reduces validation loss) or detrimental (a negative contribution might increase validation loss).\n"
      ],
      "metadata": {
        "id": "bzpaM4x0rTrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What more to do\n",
        "\n",
        "1. plot the contribution graph over time to segregate positive and negative contribution on this\n",
        "2. maybe remove the negative ones and see how that affects performance\n",
        "\n",
        "\n",
        "\n",
        "# ALSO -\n",
        "\n",
        "- Dynamic Re-weighting: Use the Shapley scores as dynamic “weights” for each training sample in the objective, so that negative contributors are downweighted in real time.\n",
        "\n",
        "- Curriculum Learning: Potentially design a curriculum where you first train on high-contribution samples, then gradually introduce lower-contribution samples.\n",
        "\n",
        "- Subset Selection: If you have a very large dataset, you might only train on the subset of samples with the highest Shapley contribution, speeding up training but still achieving good performance on your validation set."
      ],
      "metadata": {
        "id": "CHDWopuzrfxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe some sorting\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "sorted_shite = torch.argsort(data_shapley_scores)\n",
        "\n",
        "print(\"HIghest shapley scores\")\n",
        "for rank in range(10):\n",
        "    idx = sorted_shite[rank].item()        # index in the dataset\n",
        "    val = data_shapley_scores[idx].item()    # Shapley score\n",
        "    print(f\"Rank {rank+1:2d} | Sample Index: {idx:5d} | Score: {val:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Lowest shapley scores\")\n",
        "for rank in range(10):\n",
        "    idx = sorted_shite[-(rank+1)].item()   # index from the end\n",
        "    val = data_shapley_scores[idx].item()    # Shapley score\n",
        "    print(f\"Rank {rank+1:2d} | Sample Index: {idx:5d} | Score: {val:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctTjv_P8pNKk",
        "outputId": "0f2149c9-2146-4220-858a-830648addf04"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIghest shapley scores\n",
            "Rank  1 | Sample Index: 46542 | Score: -3.9043\n",
            "Rank  2 | Sample Index: 48812 | Score: -3.8358\n",
            "Rank  3 | Sample Index: 39066 | Score: -3.7442\n",
            "Rank  4 | Sample Index:  7147 | Score: -3.6062\n",
            "Rank  5 | Sample Index: 49441 | Score: -3.5339\n",
            "Rank  6 | Sample Index: 44791 | Score: -3.5203\n",
            "Rank  7 | Sample Index: 45104 | Score: -3.4897\n",
            "Rank  8 | Sample Index: 35108 | Score: -3.4149\n",
            "Rank  9 | Sample Index: 43943 | Score: -3.4097\n",
            "Rank 10 | Sample Index: 51668 | Score: -3.4073\n",
            "Lowest shapley scores\n",
            "Rank  1 | Sample Index: 34313 | Score: 2.5663\n",
            "Rank  2 | Sample Index: 59127 | Score: 2.4685\n",
            "Rank  3 | Sample Index: 55037 | Score: 2.4204\n",
            "Rank  4 | Sample Index: 27612 | Score: 2.3866\n",
            "Rank  5 | Sample Index: 27596 | Score: 2.3051\n",
            "Rank  6 | Sample Index: 51275 | Score: 2.2888\n",
            "Rank  7 | Sample Index: 52996 | Score: 2.2684\n",
            "Rank  8 | Sample Index: 29027 | Score: 2.2673\n",
            "Rank  9 | Sample Index: 45182 | Score: 2.1143\n",
            "Rank 10 | Sample Index: 50776 | Score: 2.0524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gVGGNrADtEvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}